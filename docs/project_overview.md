# Tale - AI 영상 생성 파이프라인

> 작성일: 2026-02-09 | 대상: 팀원, 디자이너, 비개발 직군

---

## 한 줄 요약

**음악이나 스토리를 넣으면 AI가 영화처럼 촬영된 영상을 만들어주는 시스템.**

---

## 무엇을 하는 프로젝트인가?

Tale은 텍스트(스토리, 가사, 설정 등)나 음악 정보를 입력하면,
AI가 자동으로 **씬을 나누고, 카메라 앵글을 잡고, 영상을 생성**하는 파이프라인입니다.

일반적인 AI 영상 생성 도구(Runway, Pika 등)가 "한 장면 한 장면" 만드는 것과 달리,
Tale은 **전체 영상의 흐름을 설계**한 뒤 각 장면을 만듭니다.

### 비유하면

| 기존 AI 영상 도구 | Tale |
|---|---|
| 사진작가에게 한 장씩 요청 | **감독 + 촬영팀**에게 영상 전체를 맡기기 |
| "비 오는 거리를 걷는 사람" → 8초 클립 1개 | "슬픈 노래" → 2분짜리 뮤직비디오 |

---

## 핵심 개념 3가지

### 1. 3단계 파이프라인 (감독의 작업 과정)

```
입력: 스토리 또는 음악
         │
         ▼
   ┌──────────────┐
   │  1단계: 감독  │  스토리를 장면(Scene)으로 나눈다
   │  Scene 설계   │  "이 영상은 3개 장면으로 구성하자"
   └──────────────┘
         │
         ▼
   ┌──────────────┐
   │  2단계: 촬영  │  각 장면을 샷(Shot)으로 쪼갠다
   │  Shot 구성    │  "이 장면은 와이드 → 클로즈업 → 오버숄더"
   └──────────────┘
         │
         ▼
   ┌──────────────┐
   │  3단계: 조감  │  각 샷에 촬영 기법을 입힌다
   │  프롬프트 생성 │  "핸드헬드 + 키아로스쿠로 조명 + 필름 그레인"
   └──────────────┘
         │
         ▼
   최종 영상 (8초 클립 × 15개 = 약 2분)
```

- **1단계 (Scene Architect)**: 전체 스토리를 시간/장소별로 장면 분할
- **2단계 (Shot Composer)**: 각 장면 안에서 카메라 앵글, 구도, 대사 결정
- **3단계 (Prompt Builder)**: 실제 AI에게 보낼 영상 생성 지시문 작성

### 2. AVA Framework (다양한 입력을 영상으로 바꾸는 방법)

음악도 되고, 소설도 되고, 게임 설정도 되는 이유는 **AVA**라는 변환 프레임워크 덕분입니다.

```
   음악 ─┐
         │     ┌──────────┐     ┌──────────┐     ┌──────────┐
   소설 ─┼──▶  │  Anchor   │──▶ │  Bridge   │──▶ │Expression│──▶ 영상
         │     │ (핵심 DNA) │    │ (번역 전략)│    │(시각 표현)│
  게임 ──┘     └──────────┘     └──────────┘     └──────────┘
```

| 단계 | 역할 | 예시 (슬픈 노래 입력 시) |
|------|------|---|
| **Anchor** | 입력에서 본질 추출 | 감정: 우울, 템포: 느림, 서사: 상실 |
| **Bridge** | 본질을 시각으로 번역 | 우울 → 비 내리는 도시, 황혼 |
| **Expression** | 구체적 영상 요소 결정 | 핸드헬드 카메라, 세피아 톤, 느린 움직임 |

### 3. Knowledge DB (촬영 기법 사전)

영화/뮤직비디오에서 쓰이는 촬영 기법 30개가 DB에 정리되어 있고,
AI가 장면의 분위기에 맞는 기법을 자동으로 골라 씁니다.

| 카테고리 | 예시 | 설명 |
|----------|------|------|
| 카메라 언어 | Handheld, Vertigo, Steadicam | 카메라를 어떻게 움직이는가 |
| 렌더링 스타일 | Chiaroscuro, Neon Noir, Oil Painting | 화면이 어떤 느낌인가 |
| 샷 문법 | Silhouette Reveal, Push-in Realization | 어떤 연출 패턴을 쓰는가 |

각 기법에는 **감정 태그**가 달려 있어서, "긴장감 있는 장면"에는 자동으로 Handheld + Dutch Angle 같은 기법이 선택됩니다.

---

## 실제 동작 예시

### 입력: 음악 메타데이터

```
곡: "Rainy Memories"
BPM: 72 (느린 템포)
분위기: melancholic, nostalgic
구간: intro(30초) → verse(60초) → chorus(60초) → outro(30초)
```

### 출력 과정

```
1. AVA 변환
   → 감정: 우울 + 향수
   → 장소: 비 내리는 도시, 오래된 카페
   → 시간: 황혼
   → 카메라: Steadicam (부드러운 움직임)
   → 스타일: Film Grain 70s (빈티지 필름 느낌)

2. 씬 분할 (3개 장면)
   → Scene 1: 빗속 거리를 걷는 인물 (intro~verse 전반)
   → Scene 2: 카페 안에서 추억 회상 (verse 후반~chorus)
   → Scene 3: 다시 거리, 빗줄기 사이로 걸어감 (outro)

3. 샷 구성 (15개 샷)
   → Shot 1: Wide Shot - 빗속 도시 전경
   → Shot 2: Medium Shot - 인물 등장, 우산 없이 걷는 중
   → Shot 3: Close Up - 빗방울이 떨어지는 얼굴
   → ... (이하 12개 샷)

4. 최종 영상 생성
   → 각 샷별로 AI(Veo)가 8초 영상 클립 생성
   → 총 15개 클립 = 약 2분 영상
```

---

## 사용하는 외부 AI 서비스

| 용도 | 서비스 | 역할 |
|------|--------|------|
| 스토리 구성, 씬 분할 | Google Gemini | 텍스트 분석 및 생성 (감독 역할) |
| 영상 생성 | Google Veo | 텍스트/이미지 → 8초 영상 클립 |
| 캐릭터 이미지 | OpenAI DALL-E 3 | 캐릭터 레퍼런스 이미지 생성 |
| DB | Supabase | 촬영 기법 DB, 영상 레퍼런스 DB |

---

## 현재 상태 (2026-02 기준)

### 완성된 것
- 3단계 파이프라인 (Scene → Shot → Prompt) 동작
- AVA Framework (음악 → 영상 변환)
- Knowledge DB (촬영 기법 30개)
- Video Reference DB (영상 레퍼런스 수집/분석 구조)
- 테스트 154개, 커버리지 93%

### 아직 남은 것
- 실제 음악 → 영상 End-to-End 테스트
- Symbolic/Sensory 모드 (현재 Intuitive만 지원)
- 영상 레퍼런스 데이터 수집
- 웹 UI (현재 CLI만 지원)
- 사용자 인증, 과금 시스템

---

## 용어 사전

| 용어 | 뜻 |
|------|------|
| **Scene** | 시간/장소가 같은 하나의 장면 (영화의 "씬") |
| **Shot** | 카메라 한 번 돌리는 단위 = AI가 만드는 8초 영상 1개 |
| **Anchor** | 입력(음악/스토리)에서 뽑아낸 핵심 요소 (감정, 서사, 구조) |
| **Bridge** | Anchor를 시각 요소로 바꾸는 전략 |
| **Expression** | 최종 영상에 반영될 시각적 표현 (장소, 분위기, 카메라워크) |
| **Knowledge DB** | 촬영 기법 레퍼런스 데이터베이스 |
| **Pumpup** | 짧은 스토리를 영상화하기 좋게 구체적으로 확장하는 과정 |
| **T2V** | Text-to-Video (텍스트 → 영상) |
| **I2V** | Image-to-Video (이미지 → 영상, 캐릭터 일관성용) |
